knitr::opts_chunk$set(echo = TRUE)
gpa <- read.table("CH01PR19.txt", header = FALSE)
names(gpa) <- c("Y", "X")
reg <- lm(Y~X, data = gpa)
coe <- summary(reg)$coefficients
anova(reg)
| |Sum of Squares|df|MSS|F-Stat|P-value|
| ------------- |:-------------:| -----:| -----:| -----:| -----:|
|$SS_{Regression}$|3.588|1|3.5878|9.2402|0.002917|
|$SS_{Resigual}$|45.818|118|0.3883|||
|$SS_T$|49.406|119|0.4151|||
F_stat <- (coe[2, 1]/(coe[2, 2])) ^ 2
F_ref <- qf(0.99, 1, 118)
F_stat > F_ref
F-stat
F_stat
anova(reg)
R2 <- anova(reg)[1, 2] / (anova(reg)[1, 2] + anova(reg)[2, 2])
r <- sqrt(R2)
r
reg
3.588/49.406
sqrt(0.07262276)
plastic <- read.table("CH01PR22.txt")
names(plastic) <- c("Y", "X")
reg_1 <- lm(Y~X, data = plastic)
anova(reg_1)
coe_1 <- summary(reg_1)$coefficients
F_1 <- (coe_1[2, 1]/coe_1[2, 2])^2
P_1 <- 1-pf(F_1, 1, 14)
P_1 < 0.01
R2_1 <- anova(reg_1)[1, 2] / (anova(reg_1)[1, 2] + anova(reg_1)[2, 2])
r_1 <- sqrt(R2_1)
?rnorm
set.seed(1)
err <- rnorm(5, 0, 5)
X <- seq(4, 20, 4)
Y <- 4*X + 20 + err
reg_2 <- lm(Y~X)
reg_2
X_h <- data.frame(X = 10)
predict(reg_2, X_h, se.fit = TRUE, interval = "confidence", level = 0.95)
sqrt(0.125)
b1 <- c()
CIs <- list()
for (i in 1:200) {
set.seed(i)
err <- rnorm(5, 0, 5)
X <- seq(4, 20, 4)
Y <- 4*X + 20 + err
reg <- lm(Y~X)
ce <- summary(reg)$coefficients
b1[i] <- ce[2, 1]
pre <- predict(reg_1, x_h, se.fit = TRUE, interval = "confidence", level = 0.95)$fit
CIs[[i]] <- c(pre[2], pre[3])
}
b1 <- c()
CIs <- list()
for (i in 1:200) {
set.seed(i)
err <- rnorm(5, 0, 5)
X <- seq(4, 20, 4)
Y <- 4*X + 20 + err
reg <- lm(Y~X)
ce <- summary(reg)$coefficients
b1[i] <- ce[2, 1]
pre <- predict(reg_1, X_h, se.fit = TRUE, interval = "confidence", level = 0.95)$fit
CIs[[i]] <- c(pre[2], pre[3])
}
sd(b1)
b1 <- c()
CIs <- list()
for (i in 1:200) {
err <- rnorm(5, 0, 5)
X <- seq(4, 20, 4)
Y <- 4*X + 20 + err
reg <- lm(Y~X)
ce <- summary(reg)$coefficients
b1[i] <- ce[2, 1]
pre <- predict(reg_1, X_h, se.fit = TRUE, interval = "confidence", level = 0.95)$fit
CIs[[i]] <- c(pre[2], pre[3])
}
sd(b1)
X
var(X)
sd(X)
X
for (i in 1:2000) {
set.seed(i)
err <- rnorm(5, 0, 5)
X <- seq(4, 20, 4)
Y <- 4*X + 20 + err
reg <- lm(Y~X)
ce <- summary(reg)$coefficients
b1[i] <- ce[2, 1]
pre <- predict(reg_1, X_h, se.fit = TRUE, interval = "confidence", level = 0.95)$fit
CIs[[i]] <- c(pre[2], pre[3])
}
sd(b1)
summary(reg_2)
b1
mean(b1)
p = 0
for(i in 1:200) {
if(60 >= CIs[[i]][1] && 60 <= CIs[[i]][2]) {
p = p + 1;
}
}
p / 200
p = 0
for(i in 1:200) {
if(60 >= CIs[[i]][1] && 60 <= CIs[[i]][2]) {
p = p + 1;
}
}
p / 200
b1 <- c()
CIs <- list()
for (i in 1:200) {
set.seed(i)
err <- rnorm(5, 0, 5)
X <- seq(4, 20, 4)
Y <- 4*X + 20 + err
reg <- lm(Y~X)
ce <- summary(reg)$coefficients
b1[i] <- ce[2, 1]
pre <- predict(reg_1, X_h, se.fit = TRUE, interval = "confidence", level = 0.95)$fit
CIs[[i]] <- c(pre[2], pre[3])
}
p = 0
for(i in 1:200) {
if(60 >= CIs[[i]][1] && 60 <= CIs[[i]][2]) {
p = p + 1;
}
}
p / 200
CIs
b1 <- c()
CIs <- list()
for (i in 1:200) {
set.seed(i)
err <- rnorm(5, 0, 5)
X <- seq(4, 20, 4)
Y <- 4*X + 20 + err
reg <- lm(Y~X)
ce <- summary(reg)$coefficients
b1[i] <- ce[2, 1]
pre <- predict(reg, X_h, se.fit = TRUE, interval = "confidence", level = 0.95)$fit
CIs[[i]] <- c(pre[2], pre[3])
}
p = 0
for(i in 1:200) {
if(60 >= CIs[[i]][1] && 60 <= CIs[[i]][2]) {
p = p + 1;
}
}
p / 200
ggplot(plastic, aes(y = Y - predict(reg_1), x = X)) + geom_point() + ylim(c(-25, 25))
library(ggplot2)
ggplot(plastic, aes(y = Y - predict(reg_1), x = X)) + geom_point() + ylim(c(-25, 25))
setwd("~/Desktop/semester_1/4.StatisticalComputing/lab/3")
f <- function(x) {
return (-log(x)/(x+1))
}
f(0)
f(2)
library(ggplot2)
base <- ggplot(data = data.frame(x = c(0, 6)), aes(x))
base + stat_function(fun = f)
base <- ggplot(data = data.frame(interval(1,5, "(]")), aes(x))
diff.quot <- function(x, h=0.0001) {
return ((f(x+h)-f(x))/h)
}
base + stat_function(fun = f1) + stat_function(fun = diff.quot)
base + stat_function(fun = f) + stat_function(fun = diff.quot)
base + stat_function(fun = f) + stat_function(fun = diff.quot)
basic.grad.descent <- function(x, max.iter = 10000, stop.deriv = 1e-10, h = .0001, step.scale = .5) {
iter = 0
prime = Inf
x_init = x
while((abs(prime) > stop.deriv) & (iter < max.iter)) {
prime = diff.quot(x_init, h)
x_init = x_init - step.scale * prime
iter = iter + 1
}
res = list(x_res = x_init, f1_res = f1(x_init), iter_res = iter, conv = (iter < max.iter))
return (res)
}
basic.grad.descent(1)
basic.grad.descent <- function(x, max.iter = 10000, stop.deriv = 1e-10, h = .0001, step.scale = .5) {
iter = 0
prime = Inf
x_init = x
while((abs(prime) > stop.deriv) & (iter < max.iter)) {
prime = diff.quot(x_init, h)
x_init = x_init - step.scale * prime
iter = iter + 1
}
res = list(x_res = x_init, f_res = f(x_init), iter_res = iter, conv = (iter < max.iter))
return (res)
}
basic.grad.descent(1)
nlm(f1, 1)
nlm(f, 1)
