)
model
model <- glm(Y~X1+X2+X3+X4+X5+X6+X7,
data=prostate,
family=binomial(link = "probit")
)
model
model <- glm(Y~X1+X2+X3+X4+X5+X6+X7,
data=prostate,
family=binomial(link = "logit")
)
model
coe <- as.numeric(model$coefficients)
new <- c(1, 21.3, 8.4, 48.4, 68, 4.7, 0, 3.2)
logit <- sum(coe*new)
prob <- 1 / (1 + exp(-logit))
prob
Newtons.Method <- function(f, x0, max.iter = 200, stopping.deriv = 0.001, ...) {
n <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[, 1] <- x0
for (k in 2:max.iter) {
# Calculate the gradient
grad.cur <- grad(f, xmat[ ,k-1], ...)
# Should we stop?
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
# Move in the opposite direction of the grad
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k] # Trim
return(list(x = xmat[,k], xmat = xmat, k = k))
}
new <- Newtons.Method(logistic.NLL, x0=rep(0,8))
install.packages("numDeriv")
library(numDeriv)
Newtons.Method <- function(f, x0, max.iter = 200, stopping.deriv = 0.001, ...) {
n <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[, 1] <- x0
for (k in 2:max.iter) {
# Calculate the gradient
grad.cur <- grad(f, xmat[ ,k-1], ...)
# Should we stop?
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
# Move in the opposite direction of the grad
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k] # Trim
return(list(x = xmat[,k], xmat = xmat, k = k))
}
new <- Newtons.Method(logistic.NLL, x0=rep(0,8))
new <- Newtons.Method(logistic.NLL, x0=rep(0,8), step.size = 0.05)
Newtons.Method <- function(f, x0, max.iter = 200, stopping.deriv = 0.001, ...) {
n <- length(x0)
step.size <- 0.05
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[, 1] <- x0
for (k in 2:max.iter) {
# Calculate the gradient
grad.cur <- grad(f, xmat[ ,k-1], ...)
# Should we stop?
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
# Move in the opposite direction of the grad
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k] # Trim
return(list(x = xmat[,k], xmat = xmat, k = k))
}
new <- Newtons.Method(logistic.NLL, x0=rep(0,8))
new <- Newtons.Method(logistic.NLL, x0=rep(0,8))
logistic.nlm <- nlm(logistic.NLL, p=rep(0,8), data=prostate)
logistic.nlm$estimate
logistic.nlm$iterations
?grad
Newtons.Method <- function(f, x0, max.iter = 200, step.size <- 0.05, stopping.deriv = 0.001, ...) {
Newtons.Method <- function(f, x0, max.iter = 200, step.size = 0.05, stopping.deriv = 0.001, ...) {
n <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[, 1] <- x0
for (k in 2:max.iter) {
# Calculate the gradient
grad.cur <- grad(f, xmat[ ,k-1], ...)
# Should we stop?
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
# Move in the opposite direction of the grad
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k] # Trim
return(list(x = xmat[,k], xmat = xmat, k = k))
}
new <- Newtons.Method(logistic.NLL, x0=rep(0,8), step.size = 0.1)
new <- Newtons.Method(logistic.NLL, x0=rep(0,8), step.size = 0.01)
new <- Newtons.Method(logistic.NLL, x0=rep(0,8), step.size = 0.001)
new <- Newtons.Method(logistic.NLL, x0=rep(0,8), step.size = 0.0001)
new[[1]]
new[[3]]
new <- Newtons.Method(logistic.NLL, x0=rep(0,8), step.size = 0.0005)
new[[1]]
new[[3]]
Newtons.Method <- function(f, x0, max.iter = 200, step.size = 0.05, stopping.deriv = 0.001, ...) {
n <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[, 1] <- x0
for (k in 2:max.iter) {
# Calculate the gradient
grad.cur <- grad(f, xmat[ ,k-1], ...)
# Should we stop?
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
# Move in the opposite direction of the grad
xmat[ ,k] <- xmat[ ,k-1] - 1/k * grad.cur
}
xmat <- xmat[ ,1:k] # Trim
return(list(x = xmat[,k], xmat = xmat, k = k))
}
new <- Newtons.Method(logistic.NLL, x0=rep(0,8), step.size = 0.0005)
Newtons.Method <- function(f, x0, max.iter = 200, step.size = 0.05, stopping.deriv = 0.001, ...) {
n <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[, 1] <- x0
for (k in 2:max.iter) {
# Calculate the gradient
grad.cur <- grad(f, xmat[ ,k-1], ...)
# Should we stop?
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
# Move in the opposite direction of the grad
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k] # Trim
return(list(x = xmat[,k], xmat = xmat, k = k))
}
new <- Newtons.Method(logistic.NLL, x0=rep(0,8), step.size = 0.001)
new <- Newtons.Method(logistic.NLL, x0=rep(0,8), step.size = 0.0001)
new[[1]]
new[[3]]
new <- Newtons.Method(logistic.NLL, x0=rep(0,8), step.size = 0.0002)
new[[1]]
new[[3]]
new <- Newtons.Method(logistic.NLL, x0=rep(0,8), step.size = 0.0003)
new[[1]]
new[[3]]
new <- Newtons.Method(logistic.NLL, x0=rep(0,8), step.size = 0.0006)
new[[1]]
new[[3]]
new <- Newtons.Method(logistic.NLL, x0=rep(0,8), step.size = 0.00001)
new[[1]]
new[[3]]
logistic.nlm$iterations
knitr::opts_chunk$set(echo = TRUE)
n <- 100
p <- 10
s <- 3
set.seed(0)
x <- matrix(rnorm(n*p), n, p)
b <- c(-0.7, 0.7, 1, rep(0, p-s))
y <- x %*% b + rt(n, df=2)
cor(x, y)
n <- 100
p <- 10
s <- 3
set.seed(0)
x <- matrix(rnorm(n*p), n, p)
b <- c(-0.7, 0.7, 1, rep(0, p-s))
y <- x %*% b + rnorm(n, df=2)
n <- 100
p <- 10
s <- 3
set.seed(0)
x <- matrix(rnorm(n*p), n, p)
b <- c(-0.7, 0.7, 1, rep(0, p-s))
y <- x %*% b + rnorm(n)
cor(x, y)
n <- 100
p <- 10
s <- 3
set.seed(0)
x <- matrix(rnorm(n*p), n, p)
b <- c(-0.7, 0.7, 1, rep(0, p-s))
y <- x %*% b + rt(n, df=2)
cor(x, y)
range <- seq(-4, 4, 0.01)
plot(range, dnorm(range), type = "l", ylab = "density")
lines(range, dt(range, 3), col = "blue")
legend(-4, 0.4, legend=c("standard normal", "t-distribution with df=3"), col=c("black", "blue"), pch=c(16, 16), cex = 0.8)
psi <- function(r, c = 1) {
return(ifelse(r^2 > c^2, 2*c*abs(r) - c^2, r^2))
}
huber.loss <- function(beta) {
resd <- x %*% beta - y
res <- sum(psi(resd))
return(res)
}
library(numDeriv)
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.05, stopping.deriv = 0.01, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
# Calculate the gradient
grad.cur <- grad(f, xmat[ ,k-1], ...)
# Should we stop?
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
# Move in the opposite direction of the grad
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k] # Trim
return(list(x = xmat[,k], xmat = xmat, k = k))
}
gd <- grad.descent(huber.loss, x0 = rep(0, p), step.size = 0.001, stopping.deriv = 0.1)
gd$x
gd$k
gd$xmat[, 1]
gd$xmat[, 2]
gd$xmat[, 3]
gd$xmat[, 127]
x %*% gd$xmat[, 2]
obj <- apply(gd$xmat, 2, function(b) x %*% b)
obj[1]
obj[, 1]
obj[, 2]
obj <- apply(gd$xmat, 2, huber.loss)
obj
plot(1:127, obj)
plot(1:127, obj, type = "l")
plot(1:127, obj, type = "l", xlab = "iterations", ylab = "Huber loss")
gd1 <- grad.descent(huber.loss, x0 = rep(0, p), step.size = 0.1, stopping.deriv = 0.1)
gd1 <- grad.descent(huber.loss, x0 = rep(0, p), step.size = 0.1, stopping.deriv = 0.1)
obj1 <- apply(gd1$xmat, 2, huber.loss)
obj1[, -50:-1]
obj1[, -1]
obj1[, 127]
obj1$x
gd1$k
obj[, 200]
obj1[, 200]
obj1
obj1[-1]
obj1[1:2]
obj1[-50:-1]
plot(1:127, obj1[151:200], type = "l", xlab = "iterations", ylab = "Huber loss")
plot(151:200, obj1[151:200], type = "l", xlab = "iterations", ylab = "Huber loss")
plot(1:200, obj1[1:200], type = "l", xlab = "iterations", ylab = "Huber loss")
gd1$xmat[, 150:200]
plot(151:200, obj1[151:200], type = "l", xlab = "iterations", ylab = "Huber loss")
gd$x
ifelse(abs(c(0.1, 0.2, 0.01)) < 0.05, 0, c(0.1, 0.2, 0.01))
ifelse(abs(c(0.1, 0.2, 0.01, -0.01)) < 0.05, 0, c(0.1, 0.2, 0.01))
sparse.grad.descent <- function(f, x0, max.iter = 200, step.size = 0.05, stopping.deriv = 0.01, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
# Calculate the gradient
grad.cur <- grad(f, xmat[ ,k-1], ...)
# Should we stop?
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
# Move in the opposite direction of the grad
tmp <- xmat[ ,k-1] - step.size * grad.cur
tmp <- ifelse(abs(tmp) <= 0.05, 0, tmp)
xmat[ ,k] <- tmp
}
xmat <- xmat[ ,1:k] # Trim
return(list(x = xmat[,k], xmat = xmat, k = k))
}
gd.sparse <- sparse.grad.descent(huber.loss, x0 = rep(0, p), step.size = 0.001, stopping.deriv = 0.1)
gd.sparse$x
lm(y~x)
gd$x
gd.sparse$x
as.numeric(lm(y~x)$coef)
lm_coe <- as.numeric(lm(y~x)$coef)
gd_coe <- gd$x
spa_gd_coe <- gd.sparse$x
lm_coe
gd_coe
lm_coe <- as.numeric(lm(y~x)$coef)
gd_coe <- gd$x
spa_gd_coe <- gd.sparse$x
mean((x%*%lm_coe-y)^2)
mean((x%*%gd_coe-y)^2)
mean((x%*%spa_gd_coe-y)^2)
gd_coe
mean((x%*%lm_coe-y)^2)
lm_coe
lm(y~x)
lm(y~x-1)
lm_coe <- as.numeric(lm(y~x-1)$coef)
mean((x%*%lm_coe-y)^2)
mean((x%*%lm_coe-y)^2)
mean((x%*%gd_coe-y)^2)
mean((x%*%spa_gd_coe-y)^2)
huber.loss <- function(beta, x, y) {
resd <- x %*% beta - y
res <- sum(psi(resd))
return(res)
}
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.05, stopping.deriv = 0.01, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
# Calculate the gradient
grad.cur <- grad(f, xmat[ ,k-1], ...)
# Should we stop?
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
# Move in the opposite direction of the grad
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k] # Trim
return(list(x = xmat[,k], xmat = xmat, k = k))
}
sparse.grad.descent <- function(f, x0, max.iter = 200, step.size = 0.05, stopping.deriv = 0.01, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
# Calculate the gradient
grad.cur <- grad(f, xmat[ ,k-1], ...)
# Should we stop?
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
# Move in the opposite direction of the grad
tmp <- xmat[ ,k-1] - step.size * grad.cur
tmp <- ifelse(abs(tmp) <= 0.05, 0, tmp)
xmat[ ,k] <- tmp
}
xmat <- xmat[ ,1:k] # Trim
return(list(x = xmat[,k], xmat = xmat, k = k))
}
set.seed(10)
y_new <- x %*% b + rt(n, df=2)
y_new
y
y_new <- x %*% b + rt(n, df=2)
y_new
set.seed(10)
y_new <- x %*% b + rt(n, df=2)
gd_new <- grad.descent(huber.loss, x0 = rep(0, p), step.size = 0.001, stopping.deriv = 0.1,
x = x, y = y_new)
huber.loss <- function(beta, x = x, y = y) {
resd <- x %*% beta - y
res <- sum(psi(resd))
return(res)
}
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.05, stopping.deriv = 0.01, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
# Calculate the gradient
grad.cur <- grad(f, xmat[ ,k-1], ...)
# Should we stop?
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
# Move in the opposite direction of the grad
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k] # Trim
return(list(x = xmat[,k], xmat = xmat, k = k))
}
sparse.grad.descent <- function(f, x0, max.iter = 200, step.size = 0.05, stopping.deriv = 0.01, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
# Calculate the gradient
grad.cur <- grad(f, xmat[ ,k-1], ...)
# Should we stop?
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
# Move in the opposite direction of the grad
tmp <- xmat[ ,k-1] - step.size * grad.cur
tmp <- ifelse(abs(tmp) <= 0.05, 0, tmp)
xmat[ ,k] <- tmp
}
xmat <- xmat[ ,1:k] # Trim
return(list(x = xmat[,k], xmat = xmat, k = k))
}
gd_new <- grad.descent(huber.loss, x0 = rep(0, p), step.size = 0.001, stopping.deriv = 0.1,
y = y_new)
huber.loss <- function(beta, x, y) {
resd <- x %*% beta - y
res <- sum(psi(resd))
return(res)
}
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.05, stopping.deriv = 0.01, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
# Calculate the gradient
grad.cur <- grad(f, xmat[ ,k-1], ...)
# Should we stop?
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
# Move in the opposite direction of the grad
xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
}
xmat <- xmat[ ,1:k] # Trim
return(list(x = xmat[,k], xmat = xmat, k = k))
}
sparse.grad.descent <- function(f, x0, max.iter = 200, step.size = 0.05, stopping.deriv = 0.01, ...) {
n    <- length(x0)
xmat <- matrix(0, nrow = n, ncol = max.iter)
xmat[,1] <- x0
for (k in 2:max.iter) {
# Calculate the gradient
grad.cur <- grad(f, xmat[ ,k-1], ...)
# Should we stop?
if (all(abs(grad.cur) < stopping.deriv)) {
k <- k-1; break
}
# Move in the opposite direction of the grad
tmp <- xmat[ ,k-1] - step.size * grad.cur
tmp <- ifelse(abs(tmp) <= 0.05, 0, tmp)
xmat[ ,k] <- tmp
}
xmat <- xmat[ ,1:k] # Trim
return(list(x = xmat[,k], xmat = xmat, k = k))
}
gd_new <- grad.descent(huber.loss, x0 = rep(0, p), step.size = 0.001, stopping.deriv = 0.1,
x = x, y = y_new)
huber.loss <- function(beta) {
resd <- x %*% beta - y
res <- sum(psi(resd))
return(res)
}
set.seed(10)
y <- x %*% b + rt(n, df=2)
y
gd_new <- grad.descent(huber.loss, x0 = rep(0, p), step.size = 0.001, stopping.deriv = 0.1)
gd_new$x
spa_gd_new <- sparse.grad.descent(huber.loss, x0 = rep(0, p), step.size = 0.001, stopping.deriv = 0.1)
spa_gd_new$x
mean((x%*%gd_new$x - y)^2)
mean((x%*%spa_gd_new$x - y)^2)
mean((x%*%spa_gd_new$x - y)^2)
gd_coe
mean((x%*%b - y)^2)
mean((lm_coe-b)^2)
mean((gd_coe-b)^2)
mean((spa_gd_coe-b)^2)
mean((gd_new$x - b)^2)
mean((spa_gd_new$x - b)^2)
for(i in 1:10) {
y <- x %*% b + rt(n, df=2)
gd_new <- grad.descent(huber.loss, x0 = rep(0, p), step.size = 0.001, stopping.deriv = 0.1)
mse_gd <- c(mse_gd, mean((gd_new$x - b)^2))
spa_gd_new <- sparse.grad.descent(huber.loss, x0 = rep(0, p), step.size = 0.001, stopping.deriv = 0.1)
mse_sgd <- c(mse_sgd, mean((spa_gd_new$x - b)^2))
}
mse_gd <- c()
mse_sgd <- c()
for(i in 1:10) {
y <- x %*% b + rt(n, df=2)
gd_new <- grad.descent(huber.loss, x0 = rep(0, p), step.size = 0.001, stopping.deriv = 0.1)
mse_gd <- c(mse_gd, mean((gd_new$x - b)^2))
spa_gd_new <- sparse.grad.descent(huber.loss, x0 = rep(0, p), step.size = 0.001, stopping.deriv = 0.1)
mse_sgd <- c(mse_sgd, mean((spa_gd_new$x - b)^2))
}
mse_gd
mse_sgd
mean(mse_gd)
mean(mse_sgd)
for(i in 1:10) {
y <- x %*% b + rt(n, df=2)
gd_new <- grad.descent(huber.loss, x0 = rep(0, p), step.size = 0.001, stopping.deriv = 0.1)
mse_gd <- c(mse_gd, mean((gd_new$x - b)^2))
spa_gd_new <- sparse.grad.descent(huber.loss, x0 = rep(0, p), step.size = 0.001, stopping.deriv = 0.1)
mse_sgd <- c(mse_sgd, mean((spa_gd_new$x - b)^2))
}
mean(mse_gd)
mean(mse_sgd)
min(mse_gd)
min(mse_sgd)
mse_gd
mse_gd <- c()
mse_sgd <- c()
for(i in 1:10) {
y <- x %*% b + rt(n, df=2)
gd_new <- grad.descent(huber.loss, x0 = rep(0, p), step.size = 0.001, stopping.deriv = 0.1)
mse_gd <- c(mse_gd, mean((gd_new$x - b)^2))
spa_gd_new <- sparse.grad.descent(huber.loss, x0 = rep(0, p), step.size = 0.001, stopping.deriv = 0.1)
mse_sgd <- c(mse_sgd, mean((spa_gd_new$x - b)^2))
}
mean(mse_gd)
mean(mse_sgd)
min(mse_gd)
min(mse_sgd)
mse_gd
mse_sgd
