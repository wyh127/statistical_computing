knitr::opts_chunk$set(echo = TRUE)
gpa <- read.table("CH01PR19.txt", header = FALSE)
names(gpa) <- c("Y", "X")
reg <- lm(Y~X, data = gpa)
coe <- summary(reg)$coefficients
anova(reg)
| |Sum of Squares|df|MSS|F-Stat|P-value|
| ------------- |:-------------:| -----:| -----:| -----:| -----:|
|$SS_{Regression}$|3.588|1|3.5878|9.2402|0.002917|
|$SS_{Resigual}$|45.818|118|0.3883|||
|$SS_T$|49.406|119|0.4151|||
F_stat <- (coe[2, 1]/(coe[2, 2])) ^ 2
F_ref <- qf(0.99, 1, 118)
F_stat > F_ref
F-stat
F_stat
anova(reg)
R2 <- anova(reg)[1, 2] / (anova(reg)[1, 2] + anova(reg)[2, 2])
r <- sqrt(R2)
r
reg
3.588/49.406
sqrt(0.07262276)
plastic <- read.table("CH01PR22.txt")
names(plastic) <- c("Y", "X")
reg_1 <- lm(Y~X, data = plastic)
anova(reg_1)
coe_1 <- summary(reg_1)$coefficients
F_1 <- (coe_1[2, 1]/coe_1[2, 2])^2
P_1 <- 1-pf(F_1, 1, 14)
P_1 < 0.01
R2_1 <- anova(reg_1)[1, 2] / (anova(reg_1)[1, 2] + anova(reg_1)[2, 2])
r_1 <- sqrt(R2_1)
?rnorm
set.seed(1)
err <- rnorm(5, 0, 5)
X <- seq(4, 20, 4)
Y <- 4*X + 20 + err
reg_2 <- lm(Y~X)
reg_2
X_h <- data.frame(X = 10)
predict(reg_2, X_h, se.fit = TRUE, interval = "confidence", level = 0.95)
sqrt(0.125)
b1 <- c()
CIs <- list()
for (i in 1:200) {
set.seed(i)
err <- rnorm(5, 0, 5)
X <- seq(4, 20, 4)
Y <- 4*X + 20 + err
reg <- lm(Y~X)
ce <- summary(reg)$coefficients
b1[i] <- ce[2, 1]
pre <- predict(reg_1, x_h, se.fit = TRUE, interval = "confidence", level = 0.95)$fit
CIs[[i]] <- c(pre[2], pre[3])
}
b1 <- c()
CIs <- list()
for (i in 1:200) {
set.seed(i)
err <- rnorm(5, 0, 5)
X <- seq(4, 20, 4)
Y <- 4*X + 20 + err
reg <- lm(Y~X)
ce <- summary(reg)$coefficients
b1[i] <- ce[2, 1]
pre <- predict(reg_1, X_h, se.fit = TRUE, interval = "confidence", level = 0.95)$fit
CIs[[i]] <- c(pre[2], pre[3])
}
sd(b1)
b1 <- c()
CIs <- list()
for (i in 1:200) {
err <- rnorm(5, 0, 5)
X <- seq(4, 20, 4)
Y <- 4*X + 20 + err
reg <- lm(Y~X)
ce <- summary(reg)$coefficients
b1[i] <- ce[2, 1]
pre <- predict(reg_1, X_h, se.fit = TRUE, interval = "confidence", level = 0.95)$fit
CIs[[i]] <- c(pre[2], pre[3])
}
sd(b1)
X
var(X)
sd(X)
X
for (i in 1:2000) {
set.seed(i)
err <- rnorm(5, 0, 5)
X <- seq(4, 20, 4)
Y <- 4*X + 20 + err
reg <- lm(Y~X)
ce <- summary(reg)$coefficients
b1[i] <- ce[2, 1]
pre <- predict(reg_1, X_h, se.fit = TRUE, interval = "confidence", level = 0.95)$fit
CIs[[i]] <- c(pre[2], pre[3])
}
sd(b1)
summary(reg_2)
b1
mean(b1)
p = 0
for(i in 1:200) {
if(60 >= CIs[[i]][1] && 60 <= CIs[[i]][2]) {
p = p + 1;
}
}
p / 200
p = 0
for(i in 1:200) {
if(60 >= CIs[[i]][1] && 60 <= CIs[[i]][2]) {
p = p + 1;
}
}
p / 200
b1 <- c()
CIs <- list()
for (i in 1:200) {
set.seed(i)
err <- rnorm(5, 0, 5)
X <- seq(4, 20, 4)
Y <- 4*X + 20 + err
reg <- lm(Y~X)
ce <- summary(reg)$coefficients
b1[i] <- ce[2, 1]
pre <- predict(reg_1, X_h, se.fit = TRUE, interval = "confidence", level = 0.95)$fit
CIs[[i]] <- c(pre[2], pre[3])
}
p = 0
for(i in 1:200) {
if(60 >= CIs[[i]][1] && 60 <= CIs[[i]][2]) {
p = p + 1;
}
}
p / 200
CIs
b1 <- c()
CIs <- list()
for (i in 1:200) {
set.seed(i)
err <- rnorm(5, 0, 5)
X <- seq(4, 20, 4)
Y <- 4*X + 20 + err
reg <- lm(Y~X)
ce <- summary(reg)$coefficients
b1[i] <- ce[2, 1]
pre <- predict(reg, X_h, se.fit = TRUE, interval = "confidence", level = 0.95)$fit
CIs[[i]] <- c(pre[2], pre[3])
}
p = 0
for(i in 1:200) {
if(60 >= CIs[[i]][1] && 60 <= CIs[[i]][2]) {
p = p + 1;
}
}
p / 200
ggplot(plastic, aes(y = Y - predict(reg_1), x = X)) + geom_point() + ylim(c(-25, 25))
library(ggplot2)
ggplot(plastic, aes(y = Y - predict(reg_1), x = X)) + geom_point() + ylim(c(-25, 25))
setwd("~/Desktop/semester_1/4.StatisticalComputing/lec")
F1
strikes <- read.csv("strikes.csv", as.is = TRUE)
head(strikes)
type(strikes$country)
class(strikes$country)
str(strikes$country)
mode(strikes$country)
strikes$country
dim(strikes)
year_split <- split(strikes, strikes$year)
year_split
year_split[[1]]
sapply(year_split[3, 4, 2], mean)
sapply(year_split[2, 3, 4], mean)
install.packages("plyr")
