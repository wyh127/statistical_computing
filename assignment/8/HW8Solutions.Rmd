---
title: "Homework 8 Solutions"
output: pdf_document
---

In this homework you'll explore various optimization algorithms for forming statistical estimates in linear regression.

1. Run the following code block to create synthetic regression data, with $100$ observations and $10$ predictor variables:

```{r}
n <- 100
p <- 10
s <- 3
set.seed(0)
x <- matrix(rnorm(n * p), n, p)
b <- c(-0.7, 0.7, 1, rep(0, p - s))
y <- x %*% b + rt(n, df = 2)
```

Notice that only 3 of the 10 predictor variables in total are actually relevant in predicting the response. (That is, only the first three coefficients in `b` are nonzero.) Examine the correlation coefficients between predictor variables `x` and the response `y`; would you be able to pick out each of the 3 relevant variables based on correlations alone?

```{r}
cors <- apply(x, 2, cor, y)
cors
order(abs(cors), decreasing = TRUE)
```

2. Note that the noise in the above simulation (the difference between `y` and `x %*% b`) was created from the `rt()` function, which draws t-distributed random variables. The t-distribution has thicker tails than the normal distribution, so we are more likely to see large noise terms than we would if we used a normal distribution. Verify this by plotting the normal density and the t-density on the same plot, with the latter having 3 degrees of freedom. Choose the plot ranges appropriately, and draw the densities in different colors, so that the plot is easy to read.

```{r}
xvals <- seq(-5, 5, length.out = 100)
plot(xvals, dnorm(xvals), type = "l", ylab = "Density")
curve(dt(x, df = 3), add = TRUE, col = "red")
```

3. Because we know that the noise in our regression has thicker tails than the normal distribution, we are more likely to see outliers. Hence we're going to use the Huber loss function, which is more robust to outliers:

```{r}
psi = function(r, c = 1) {
  return(ifelse(r^2 > c^2, 2*c*abs(r) - c^2, r^2))
}
```

Write a function called `huber.loss()` that takes in as an argument a coefficient vector `beta`, and returns the sum of
`psi()` applied to the residuals (from regressing `y` on `x`). `x` and `y` should not be provided as arguments, but referred to directly in the function. You may stick with the default cutoff of `c=1`. This Huber loss is going to take the place of the usual (nonrobust) linear regression loss, i.e., the sum of squares of the residuals.

```{r}
huber.loss <- function(beta) {
  sum(psi(y - x %*% beta))
}
```

4. Using the `grad.descent()` function from lecture, run gradient descent starting from `beta=rep(0,p)`, to get an estimate of the coefficients `beta` that minimize the Huber loss, when regressing `y` on `x`. Use the settings
`max.iter=200`, `step.size=0.001`, and `stopping.deriv=0.1`. Store the output of `grad.descent()` in `gd`. How many iterations did it take to converge, and what are the final coefficient estimates?

```{r}
library(numDeriv)
grad.descent <- function(f, x0, max.iter = 200, step.size = 0.05, stopping.deriv = 0.01, ...) {
  
  n    <- length(x0)
  xmat <- matrix(0, nrow = n, ncol = max.iter)
  xmat[,1] <- x0
  
  for (k in 2:max.iter) {
    # Calculate the gradient
    grad.cur <- grad(f, xmat[ ,k-1], ...) 
    
    # Should we stop?
    if (all(abs(grad.cur) < stopping.deriv)) {
      k <- k-1; break
    }
    
    # Move in the opposite direction of the grad
    xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
  }
  
  xmat <- xmat[ ,1:k] # Trim
  return(list(x = xmat[,k], xmat = xmat, k = k))
}

gd <- grad.descent(huber.loss, x0 = rep(0,p), max.iter=200, step.size=0.001, stopping.deriv=0.1)
gd$x
gd$k
```
The final estimate is given in `gd$x` and `gd$k` gives the number of iterations.

**Note: you may need to run `install.packages("numDeriv")` in order to load the `numDeriv` library.**

5. Using `gd`, construct a vector `obj` of the values objective function encountered at each step of gradient descent. Note: here the objective function for minimization is the Huber loss. Plot these values against the iteration number, to confirm that gradient descent is indeed making the objective function at each iteration. How does the progress of the algorithm compare at the start (early iterations) versus towards the end (later iterations)?

```{r}
obj <- apply(gd$xmat[, 1:gd$k], 2, huber.loss)
plot(1:gd$k, obj, xlab = "Iteration Number", ylab = "Objective Function Value", type = "l", main = "Objective Funct. Value During Gradient Descent")
```
The value of the objective function decreases sharply for the first 40 iterations or so, but then the progress slows down, with only small decreases for the final 44 iterations.

6. Rerun gradient descent as in question 4, but with `step.size=0.1`.  Compute the new criterion values across iterations, and plot the last fifty criterion values. What do you notice now? Is the criterion decreasing at each step, and has gradient descent converged at the end (settled on a single criterion value)? What can you deduce from your plot is happening to the coefficient estimates (confirm this by looking at the `xmat` values in `gd`)?

```{r}
gd2 <- grad.descent(huber.loss, x0 = rep(0,p), max.iter=200, step.size=0.1, stopping.deriv=0.1)
gd2$x
gd2$k
obj <- apply(gd2$xmat[, 1:gd2$k], 2, huber.loss)
plot((gd2$k-49):gd2$k, obj[(gd2$k- 49):gd2$k], xlab = "Iteration Number", ylab = "Objective Function Value", type = "l", main = "Objective Funct. Value During Gradient Descent")
```
The gradient descent algorithm has not converged, and it's not decreasing at each step but rather oscilatting between values of the objective function.  The coeffcieint estimates seems to be bouncing back andforth between two points.

7. Inspect the coefficients from the first gradient descent run (stored in `gd`), and compare them to the true (unknown) underlying coefficients `b` constructed in question 1. They should be pretty close for the first 3 variables, but
the next 7 are not very accurate---that is, they're not all close to 0, as they should be. In order to fix this, we're going to apply a **sparsified** version of gradient descent (formally known as proximal gradient descent). Modify the function `grad.descent()` so that at every iteration $k$, after taking a gradient step but before saving the new estimated coefficients, we threshold small values in these coefficients to zero.  Here small means less than or equal to 0.05, in absolute value. Call the new function `sparse.grad.descent()` and rerun with the same settings as in question 4, in order to produce a sparse estimate of the regression coefficients. Stores the results in `gd.sparse`. 
What are the final coefficient estimates?

```{r}
gd$x
b
```
Indeed, we see that the first three values of `gd$x` are close to the first three values of `b`, but the remaining values aren't that close to 0.
```{r}
sparse.grad.descent <- function(f, x0, max.iter = 200, step.size = 0.05, stopping.deriv = 0.01, ...) {
  
  n    <- length(x0)
  xmat <- matrix(0, nrow = n, ncol = max.iter)
  xmat[,1] <- x0
  
  for (k in 2:max.iter) {
    # Calculate the gradient
    grad.cur <- grad(f, xmat[ ,k-1], ...) 
    
    # Should we stop?
    if (all(abs(grad.cur) < stopping.deriv)) {
      k <- k-1; break
    }
    
    # Move in the opposite direction of the grad and threshold
    update                     <- xmat[ ,k-1] - step.size * grad.cur
    update[abs(update) < 0.05] <- 0
    xmat[ ,k]                  <- update
  }
  
  xmat <- xmat[ ,1:k] # Trim
  return(list(x = xmat[,k], xmat = xmat, k = k))
}

gd.sparse <- sparse.grad.descent(huber.loss, x0 = rep(0,p), max.iter=200, step.size=0.001, stopping.deriv=0.1)
gd.sparse$x
gd.sparse$k

# The below isn't necessary to be included in the homework, but it's interesting.
obj <- apply(gd.sparse$xmat[, 1:gd.sparse$k], 2, huber.loss)
plot((gd.sparse$k-49):gd.sparse$k, obj[(gd.sparse$k- 49):gd.sparse$k], xlab = "Iteration Number", ylab = "Objective Function Value", type = "l", main = "Objective Funct. Value During Gradient Descent")
```


8. Now compute estimates of the regression coefficients in the usual manner, using `lm()`. How do these compare to those from question 4, from question 7? Compute the mean squared error between each of these three estimates of the coefficients and the true coefficients `b`. Which is best?

```{r}
lm0 <- lm(y ~ -1 + x)
lm0$coef
gd$x
gd.sparse$x
```
The linear model coefficients match closest to the solution found using regular gradient descent.  This is not surprising, as there is no reason that least squares regression would provide a sparse solution.

```{r}
mse.loss <- function(beta) {
  mean((b - beta)^2)
}

mse.loss(lm0$coef)
mse.loss(gd$x)
mse.loss(gd.sparse$x)
```
Here the sparse estimate of the vector has the smallest MSE compared to the true coefficients `b`.

9. Rerun your Huber loss minimization in questions 4 and 7, but on different data. That is, just generate another copy of `y`, per the same formula as you used in question 1: `y = x %*% b + rt(n, df=2)`. How do the new coefficient estimates look from gradient descent, and sparsified gradient descent? Which has a better mean squared error when measured against the `b` used to generate data in question 1? What do you deduce about the sparse method (e.g., what does this suggest about the variability of its estimates)?

In order to ensure that your results are comparable to other students', please run the following before generating a new `y` vector:

```{r}
set.seed(10)
y <- x %*% b + rt(n, df = 2)

gd <- grad.descent(huber.loss, x0 = rep(0,p), max.iter=200, step.size=0.001, stopping.deriv=0.1)
gd$x
gd$k

gd.sparse <- sparse.grad.descent(huber.loss, x0 = rep(0,p), max.iter=200, step.size=0.001, stopping.deriv=0.1)
gd.sparse$x
gd.sparse$k

mse.loss(gd$x)
mse.loss(gd.sparse$x)
```
In this case, regular gradient descent estimate of the vector has the smallest MSE compared to the true coefficients `b`.  This suggests high variability in the sparse estimate (comparing this result to question 8).

10. Repeat the experiment from question 9, generating 10 new copies of `y`, running gradient descent and sparse gradient descent, and recording each time the mean squared errors of each of their coefficient estimates to `b`. Report the average mean squared error, for gradient descent, and its sparse
variant, over the 10 trials. Which average lower? Also report the minimum mean squared error, for the two methods, over the 10 trials. Which is lower? Is this in line with your interpretation of the variability associated with the sparse gradient descent method?

```{r}
num <- 10
sparse.MSEs <- rep(NA, num)
reg.MSEs    <- rep(NA, num)

for (i in 1:10) {
  y <- x %*% b + rt(n, df = 2)

  gd <- grad.descent(huber.loss, x0 = rep(0,p), max.iter=200, step.size=0.001, stopping.deriv=0.1)
  
  gd.sparse <- sparse.grad.descent(huber.loss, x0 = rep(0,p), max.iter=200, step.size=0.001, stopping.deriv=0.1)
  
  reg.MSEs[i]    <- mse.loss(gd$x)
  sparse.MSEs[i] <- mse.loss(gd.sparse$x)
}

mean(reg.MSEs)
mean(sparse.MSEs)
min(reg.MSEs)
min(sparse.MSEs)
```
The minimum of the MSEs with the sparse estimates is much smaller, but the mean of the MSEs for these estimates is larger.  This supports our interpretation -- that the sparse estimate MSE has high variance.

